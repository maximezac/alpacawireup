name: Backfill History (prices + news)
on:
  workflow_dispatch:    # manual only so you control when big jobs run

permissions:
  contents: read

env:
  # Base IO paths
  INPUT_BASE: data/prices_backfill.json
  OUTPUT_BASE: data/prices_backfill.json
  # Backfill price depth (daily)
  BACKFILL_DAYS_BACK: "1500"       # ~6 yrs; your fetch_prices.py respects this as DAYS_BACK
  # News backfill knobs (default)
  WINDOW_DAYS: "30"                # 30-day windows, sliding from BACKFILL_START → BACKFILL_END
  NEWS_MAX_ARTICLES_TOTAL: "0"     # 0 = no per-symbol cap (let fetch_news keep everything)
  NEWS_LOOKBACK_DAYS: "30"         # for Finnhub / NewsAPI; matches window
  # Symbols config (trimmed backtest set)
  SYMBOLS_PATH: data/symbols_backtest.txt

jobs:
  backfill:
    runs-on: ubuntu-latest
    timeout-minutes: 1200
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.2.2 numpy==1.26.4 requests==2.32.3 \
            python-dateutil==2.9.0.post0 feedparser==6.0.11 \
            vaderSentiment==3.3.2 pyyaml==6.0.2
          pip install -r requirements.txt || echo "requirements.txt already satisfied."

      - name: Export PYTHONPATH
        run: echo "PYTHONPATH=$PWD" >> "$GITHUB_ENV"

      # ------------------------------------------------------------
      # 0) Backfill prices (multi-year)
      # ------------------------------------------------------------
      - name: Backfill prices (multi-year)
        env:
          ALPACA_KEY_ID:      ${{ secrets.ALPACA_KEY_ID }}
          ALPACA_SECRET_KEY:  ${{ secrets.ALPACA_SECRET_KEY }}
          ALPACA_DATA_FEED:   iex
          SYMBOLS_PATH:       ${{ env.SYMBOLS_PATH }}
          OUTPUT_PATH:        ${{ env.INPUT_BASE }}
          DAYS_BACK:          ${{ env.BACKFILL_DAYS_BACK }}
          # Keep ALL daily bars fetched (up to DAYS_BACK); 0 = no cap in fetch_prices.py
          DAILY_BARS_MAX:     "0"
          MAX_SYMBOLS:        "2000"
          INTRADAY_TIMEFRAME: "5Min"
          INTRADAY_DAYS_BACK: "180"
          INTRADAY_BARS_MAX:  "500"
        run: |
          mkdir -p data
          python fetch_prices.py
          echo "Quick sanity:"
          python - << 'PY'
          import json, pathlib
          p = pathlib.Path("data/prices_backfill.json")
          print("Exists:", p.exists(), "Size MB:", p.stat().st_size / (1024*1024) if p.exists() else None)
          if not p.exists():
              raise SystemExit(1)
          obj = json.loads(p.read_text())
          print("as_of_utc:", obj.get("as_of_utc"))
          syms = obj.get("symbols", {})
          print("symbols:", len(syms))
          samp = next(iter(syms.keys()), None)
          if samp:
              bars = syms[samp].get("bars", [])
              print("sample symbol:", samp)
              print("  daily bars:", len(bars))
              if bars:
                  print("  first bar:", bars[0].get("t"))
                  print("  last  bar:", bars[-1].get("t"))
          PY

      # ------------------------------------------------------------
      # Derive backfill date range directly from prices_backfill.json
      # This guarantees the news windows exactly cover price history.
      # ------------------------------------------------------------
      - name: Derive backfill date range from prices_backfill.json
        id: derive_dates
        run: |
          python - <<'PY'
          import json, pathlib, os, sys
          p = pathlib.Path("data/prices_backfill.json")
          if not p.exists():
              print("ERROR: data/prices_backfill.json not found", file=sys.stderr)
              sys.exit(1)
          obj = json.loads(p.read_text(encoding='utf-8'))
          min_date = None
          max_date = None
          for node in (obj.get("symbols") or {}).values():
              for b in node.get("bars") or []:
                  t = b.get("t")
                  if not t:
                      continue
                  d = t.split("T", 1)[0]
                  if min_date is None or d < min_date:
                      min_date = d
                  if max_date is None or d > max_date:
                      max_date = d
          if min_date is None:
              print("ERROR: no bars found in data/prices_backfill.json", file=sys.stderr)
              sys.exit(1)
          # Write outputs via GITHUB_OUTPUT so they become step outputs
          out = os.environ.get('GITHUB_OUTPUT')
          if not out:
              print("ERROR: GITHUB_OUTPUT not set", file=sys.stderr)
              sys.exit(1)
          with open(out, 'a') as f:
              f.write(f"start={min_date}\n")
              f.write(f"end={max_date}\n")
          print(f"Derived BACKFILL_START={min_date}, BACKFILL_END={max_date}")
          PY

      - name: Export computed dates to env (for subsequent steps)
        run: |
          echo "BACKFILL_START=${{ steps.derive_dates.outputs.start }}" >> "$GITHUB_ENV"
          echo "BACKFILL_END=${{ steps.derive_dates.outputs.end }}" >> "$GITHUB_ENV"
          echo "Using BACKFILL_START=${BACKFILL_START}, BACKFILL_END=${BACKFILL_END}"

      # ------------------------------------------------------------
      # 2) NEWS BACKFILL LOOP (WINDOW_DAYS windows from BACKFILL_START → BACKFILL_END)
      # Each iteration reads + writes the SAME prices_backfill.json file and appends news.
      # ------------------------------------------------------------
      - name: Backfill news history (window loop)
        env:
          INPUT_PATH:   ${{ env.INPUT_BASE }}
          OUTPUT_PATH:  ${{ env.OUTPUT_BASE }}
          NEWS_LOOKBACK_DAYS: ${{ env.NEWS_LOOKBACK_DAYS }}
          NEWS_MAX_ARTICLES_TOTAL: ${{ env.NEWS_MAX_ARTICLES_TOTAL }}
          NEWS_SOURCES: "benzinga,mtnewswires,google_rss,finnhub,reddit"
          NEWS_BACKFILL: "1"
          BACKFILL_START: ${{ env.BACKFILL_START }}
          BACKFILL_END:   ${{ env.BACKFILL_END }}
          WINDOW_DAYS:    ${{ env.WINDOW_DAYS }}
          # Increase per-call limit to attempt getting as many Alpaca items as provider allows
          NEWS_LIMIT_PER_SYMBOL: "50"
          ALPACA_KEY_ID:     ${{ secrets.ALPACA_KEY_ID }}
          ALPACA_SECRET_KEY: ${{ secrets.ALPACA_SECRET_KEY }}
          NEWSAPI_KEY:       ${{ secrets.NEWSAPI_KEY }}
          FINNHUB_KEY:       ${{ secrets.FINNHUB_KEY }}
          USE_FINBERT: "1"
          FINBERT_TRIGGER: "0.20"
          FINBERT_FRACTION: "0.30"
        run: |
          set -euo pipefail
          # Helper: add days to ISO date (UTC)
          add_days() {
            local date="$1"
            local days="$2"
            date -u -d "$date + $days day" +%F
          }
          start="$BACKFILL_START"
          end="$BACKFILL_END"
          window="$WINDOW_DAYS"
          echo "[INFO] News backfill from $start → $end (window=$window days)"
          current="$start"
          i=0
          while true; do
            # window_end is min(current + window, end)
            window_end=$(add_days "$current" "$window")
            # Clamp to BACKFILL_END
            if [[ "$(date -u -d "$window_end" +%s)" -gt "$(date -u -d "$end" +%s)" ]]; then
              window_end="$end"
            fi
            echo ""
            echo "=== Window $i: $current → $window_end ==="
            # inclusive end-of-day timestamps (so we don't drop that day's news)
            export NEWS_START_ISO="${current}T00:00:00Z"
            export NEWS_END_ISO="${window_end}T23:59:59Z"
            echo "[DEBUG] NEWS_START_ISO=${NEWS_START_ISO} NEWS_END_ISO=${NEWS_END_ISO}"
            python scripts/fetch_news.py
            # Break when we reach the final endpoint
            if [[ "$window_end" == "$end" ]]; then
              break
            fi
            # Advance current by window days
            current=$(add_days "$current" "$window")
            i=$((i+1))
          done
          echo "[INFO] Finished news backfill loop."

      # ------------------------------------------------------------
      # 3) Sanity check: count total news items for a sample symbol
      # ------------------------------------------------------------
      - name: Sanity check backfilled news
        run: |
          python - << 'PY'
          import json, pathlib
          p = pathlib.Path("data/prices_backfill.json")
          obj = json.loads(p.read_text())
          syms = obj.get("symbols", {})
          print("Total symbols:", len(syms))
          sample = "AAPL"
          if sample in syms:
              n = len(syms[sample].get("news", []))
              print(f"{sample} news items:", n)
          total_news = 0
          for node in syms.values():
              total_news += len(node.get("news", []))
          print("Total news items across all symbols:", total_news)
          PY

      # ------------------------------------------------------------
      # 4) Compress backfill file before uploading
      # ------------------------------------------------------------
      - name: Compress backfill JSON
        run: |
          cd data
          tar -czf prices_backfill.tar.gz prices_backfill.json
          ls -lh prices_backfill*

      # ------------------------------------------------------------
      # 5) Upload backfill artifact
      # ------------------------------------------------------------
      - name: Upload backfill artifact
        uses: actions/upload-artifact@v4
        with:
          name: prices-backfill
          path: |
            data/prices_backfill.json
            data/prices_backfill.tar.gz
          retention-days: 90

      - name: Cache backfill dataset
        uses: actions/cache@v4
        with:
          path: data/prices_backfill.tar.gz
          key: backfill-v1-${{ hashFiles('data/symbols_backtest.txt') }}